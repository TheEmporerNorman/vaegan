{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BLLIKAz5-Ia8",
    "outputId": "e8c32faf-6814-4587-eda4-43d2041004c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.7.0-cp37-cp37m-manylinux2010_x86_64.whl (489.6 MB)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /opt/conda3/lib/python3.7/site-packages (from tensorflow) (1.21.2)\n",
      "Collecting libclang>=9.0.1\n",
      "  Downloading libclang-12.0.0-2-py2.py3-none-manylinux1_x86_64.whl (13.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.3 MB 2.7 MB/s eta 0:00:01MB 2.7 MB/s eta 0:00:04█████                | 6.7 MB 2.7 MB/s eta 0:00:03█▉           | 8.7 MB 2.7 MB/s eta 0:00:021\n",
      "\u001b[?25hCollecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.19.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 34.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.8,~=2.7.0rc0\n",
      "  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
      "\u001b[K     |████████████████████████████████| 463 kB 27.6 MB/s eta 0:00:01��████████████████▋         | 327 kB 27.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.13.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (79 kB)\n",
      "\u001b[K     |████████████████████████████████| 79 kB 4.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.21.0\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.23.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 37.1 MB/s eta 0:00:01K     |███████████▎                    | 727 kB 37.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.32.0 in /opt/conda3/lib/python3.7/site-packages (from tensorflow) (0.37.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda3/lib/python3.7/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting keras<2.8,>=2.7.0rc0\n",
      "  Downloading keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 18.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard~=2.6\n",
      "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.8 MB 12.3 MB/s eta 0:00:01      | 1.1 MB 12.3 MB/s eta 0:00:01     |██████████████████▊             | 3.4 MB 12.3 MB/s eta 0:00:01 |████████████████████████████▎   | 5.1 MB 12.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting flatbuffers<3.0,>=1.12\n",
      "  Downloading flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.43.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.1 MB 13.3 MB/s eta 0:00:01  | 307 kB 13.3 MB/s eta 0:00:01  | 2.3 MB 13.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda3/lib/python3.7/site-packages (from tensorflow) (3.10.0.0)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 621 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 1.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=0.4.0\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "\u001b[K     |████████████████████████████████| 126 kB 24.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gast<0.5.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.6.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.1 MB 17.0 MB/s eta 0:00:01  | 471 kB 17.0 MB/s eta 0:00:012.4 MB 17.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cached-property\n",
      "  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 1.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-2.0.2-py3-none-any.whl (288 kB)\n",
      "\u001b[K     |████████████████████████████████| 288 kB 28.7 MB/s eta 0:00:01██████████████████████▉        | 215 kB 28.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.3.3-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 26.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /opt/conda3/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow) (57.4.0)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.9 MB 10.3 MB/s eta 0:00:01    | 890 kB 10.3 MB/s eta 0:00:01�█▏             | 2.8 MB 10.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 30.9 MB/s eta 0:00:01      | 348 kB 30.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda3/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow) (2.26.0)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 26.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda3/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda3/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.5.0)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 2.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /opt/conda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.26.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda3/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4847 sha256=a7d97532f2a991032fb764c5defc30f20f63f6c760d5679fcc5004394afd9171\n",
      "  Stored in directory: /home/michael.norman/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built termcolor\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, cachetools, requests-oauthlib, google-auth, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, protobuf, markdown, grpcio, google-auth-oauthlib, cached-property, absl-py, wrapt, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "\u001b[33m  WARNING: The scripts pyrsa-decrypt, pyrsa-encrypt, pyrsa-keygen, pyrsa-priv2pub, pyrsa-sign and pyrsa-verify are installed in '/home/michael.norman/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script markdown_py is installed in '/home/michael.norman/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script google-oauthlib-tool is installed in '/home/michael.norman/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script tensorboard is installed in '/home/michael.norman/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The scripts estimator_ckpt_converter, import_pb_to_tensorboard, saved_model_cli, tensorboard, tf_upgrade_v2, tflite_convert, toco and toco_from_protos are installed in '/home/michael.norman/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed absl-py-1.0.0 astunparse-1.6.3 cached-property-1.5.2 cachetools-4.2.4 flatbuffers-2.0 gast-0.4.0 google-auth-2.3.3 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.43.0 h5py-3.6.0 keras-2.7.0 keras-preprocessing-1.1.2 libclang-12.0.0 markdown-3.3.6 opt-einsum-3.3.0 protobuf-3.19.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.8 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.7.0 tensorflow-estimator-2.7.0 tensorflow-io-gcs-filesystem-0.23.1 termcolor-1.1.0 werkzeug-2.0.2 wrapt-1.13.3\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "TfkrNqTgN511"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/local/michael.norman/ipykernel_2793168/3656517534.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "icNFNrNoN-3u"
   },
   "outputs": [],
   "source": [
    "#Variables:\n",
    "MODEL_TYPE            = 2 #<-- Model Type to Employ (0 = Standard VAE, 1 = Standard GAN, 2 = VAE/GAN, 3 = ADVAE)\n",
    "DEVICE_NUM            = 0 #<-- GPU Number to use, ignore on COLAB.\n",
    "NUM_TRAINING_EXAMPLES = 60000 #<-- Number of training examples.\n",
    "NUM_TESTING_EXAMPLES  = 10000 #<-- Number of testing examples.\n",
    "BATCH_SIZE            = 256   #<-- Batch Size\n",
    "NUM_EPOCHS            = 50   #<-- Number of training epochs\n",
    "NUM_LATENT_DIM        = 100   #<-- Latent Space Size\n",
    "VAEGAN_LAYER          = 1     #<-- Used in VAE/GAN -- layer to compare latent space\n",
    "NUM_PLOT              = 16    #<-- Number of examples to plot at each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MXe05gwbOElP"
   },
   "outputs": [],
   "source": [
    "#Derived Parameters:\n",
    "input_size = [BATCH_SIZE, NUM_LATENT_DIM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ou2wIQsbOEft"
   },
   "outputs": [],
   "source": [
    "discriminators = {\"VAE\": \n",
    "                    tf.keras.Sequential(\n",
    "                    [\n",
    "                      tf.keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
    "                                                       input_shape=[28, 28, 1]),\n",
    "                      tf.keras.layers.LeakyReLU(),\n",
    "                      tf.keras.layers.Dropout(0.3),\n",
    "\n",
    "                      tf.keras.layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'),\n",
    "                      tf.keras.layers.LeakyReLU(),\n",
    "                      tf.keras.layers.Dropout(0.3),\n",
    "\n",
    "                      tf.keras.layers.Flatten(),\n",
    "                      tf.keras.layers.Dense(1)\n",
    "                    ]),\n",
    "                  \"GAN\": \n",
    "                    tf.keras.Sequential(\n",
    "                    [\n",
    "                      tf.keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
    "                                                       input_shape=[28, 28, 1]),\n",
    "                      tf.keras.layers.LeakyReLU(),\n",
    "                      tf.keras.layers.Dropout(0.3),\n",
    "\n",
    "                      tf.keras.layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'),\n",
    "                      tf.keras.layers.LeakyReLU(),\n",
    "                      tf.keras.layers.Dropout(0.3),\n",
    "\n",
    "                      tf.keras.layers.Flatten(),\n",
    "                      tf.keras.layers.Dense(1)\n",
    "                    ]), \n",
    "                  \"VAEGAN\":\n",
    "                    tf.keras.Sequential(\n",
    "                    [\n",
    "                      tf.keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
    "                                                 input_shape=[28, 28, 1], activation = tf.keras.layers.ReLU(), name = \"test_layer\"),\n",
    "                      tf.keras.layers.Dropout(0.3),\n",
    "                      tf.keras.layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same', activation = tf.keras.layers.ReLU()),\n",
    "                      tf.keras.layers.Dropout(0.3),\n",
    "                      tf.keras.layers.Flatten(),\n",
    "                      tf.keras.layers.Dense(1)\n",
    "                    ]),\n",
    "                    \"ADVAE\":\n",
    "                    tf.keras.Sequential(\n",
    "                    [\n",
    "                      tf.keras.layers.Dense(128, input_shape=[NUM_LATENT_DIM, 1],),\n",
    "                      tf.keras.layers.Dense(64),\n",
    "                      tf.keras.layers.Dense(1)\n",
    "                    ])\n",
    "                  }\n",
    "\n",
    "encoders       = {\"VAE\": \n",
    "                        tf.keras.Sequential(\n",
    "                        [\n",
    "                            tf.keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
    "                                                      input_shape=[28, 28, 1], activation = tf.keras.layers.ReLU(), name = \"test_layer\"),\n",
    "                            tf.keras.layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same', activation = tf.keras.layers.ReLU()),\n",
    "                            tf.keras.layers.Flatten(),\n",
    "                            # No activation\n",
    "                            tf.keras.layers.Dense(NUM_LATENT_DIM + NUM_LATENT_DIM),\n",
    "                        ]\n",
    "                        ),\n",
    "                  \"GAN\": \n",
    "                    tf.keras.Sequential(\n",
    "                        [\n",
    "                            tf.keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
    "                                                      input_shape=[28, 28, 1], activation = tf.keras.layers.ReLU(), name = \"test_layer\"),\n",
    "                            tf.keras.layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same', activation = tf.keras.layers.ReLU()),\n",
    "                            tf.keras.layers.Flatten(),\n",
    "                            # No activation\n",
    "                            tf.keras.layers.Dense(NUM_LATENT_DIM + NUM_LATENT_DIM),\n",
    "                        ]\n",
    "                        ), \n",
    "                  \"VAEGAN\":\n",
    "                    tf.keras.Sequential(\n",
    "                        [\n",
    "                            tf.keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
    "                                                      input_shape=[28, 28, 1], activation = tf.keras.layers.ReLU(), name = \"test_layer\"),\n",
    "                            tf.keras.layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same', activation = tf.keras.layers.ReLU()),\n",
    "                            tf.keras.layers.Flatten(),\n",
    "                            # No activation\n",
    "                            tf.keras.layers.Dense(NUM_LATENT_DIM + NUM_LATENT_DIM),\n",
    "                        ]\n",
    "                        ),\n",
    "                    \"ADVAE\":\n",
    "                    tf.keras.Sequential(\n",
    "                        [\n",
    "                            tf.keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
    "                                                      input_shape=[28, 28, 1], activation = tf.keras.layers.ReLU(), name = \"test_layer\"),\n",
    "                            tf.keras.layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same', activation = tf.keras.layers.ReLU()),\n",
    "                            tf.keras.layers.Flatten(),\n",
    "                            # No activation\n",
    "                            tf.keras.layers.Dense(NUM_LATENT_DIM),\n",
    "                        ]\n",
    "                        )\n",
    "                  }\n",
    "\n",
    "decoders       = {\"VAE\": tf.keras.Sequential(\n",
    "                        [\n",
    "                            tf.keras.layers.InputLayer(input_shape=(NUM_LATENT_DIM,)),\n",
    "                            tf.keras.layers.Dense(units=7*7*32, activation=tf.nn.relu),\n",
    "                            tf.keras.layers.Reshape(target_shape=(7, 7, 32)),\n",
    "                            tf.keras.layers.Conv2DTranspose(\n",
    "                                filters=64, kernel_size=3, strides=2, padding='same',\n",
    "                                activation='relu'),\n",
    "                            tf.keras.layers.Conv2DTranspose(\n",
    "                                filters=32, kernel_size=3, strides=2, padding='same',\n",
    "                                activation='relu'),\n",
    "                            # No activation\n",
    "                            tf.keras.layers.Conv2DTranspose(\n",
    "                                filters=1, kernel_size=3, strides=1, padding='same'),\n",
    "                        ]),\n",
    "                      \"GAN\": \n",
    "                    \n",
    "                        tf.keras.Sequential([\n",
    "                            tf.keras.layers.InputLayer(input_shape=(NUM_LATENT_DIM,)),\n",
    "                            tf.keras.layers.Dense(units=7*7*32, activation=tf.nn.relu),\n",
    "                            tf.keras.layers.Reshape(target_shape=(7, 7, 32)),\n",
    "                            tf.keras.layers.Conv2DTranspose(\n",
    "                                filters=64, kernel_size=3, strides=2, padding='same',\n",
    "                                activation='relu'),\n",
    "                            tf.keras.layers.Conv2DTranspose(\n",
    "                                filters=32, kernel_size=3, strides=2, padding='same',\n",
    "                                activation='relu'),\n",
    "                            # No activation\n",
    "                            tf.keras.layers.Conv2DTranspose(\n",
    "                                filters=1, kernel_size=3, strides=1, padding='same'),\n",
    "                        ]), \n",
    "                  \"VAEGAN\":\n",
    "                        tf.keras.Sequential([\n",
    "                            tf.keras.layers.InputLayer(input_shape=(NUM_LATENT_DIM,)),\n",
    "                            tf.keras.layers.Dense(units=7*7*32, activation=tf.nn.relu),\n",
    "                            tf.keras.layers.Reshape(target_shape=(7, 7, 32)),\n",
    "                            tf.keras.layers.Conv2DTranspose(\n",
    "                                filters=64, kernel_size=3, strides=2, padding='same',\n",
    "                                activation='relu'),\n",
    "                            tf.keras.layers.Conv2DTranspose(\n",
    "                                filters=32, kernel_size=3, strides=2, padding='same',\n",
    "                                activation='relu'),\n",
    "                            # No activation\n",
    "                            tf.keras.layers.Conv2DTranspose(\n",
    "                                filters=1, kernel_size=3, strides=1, padding='same'),\n",
    "                        ]),\n",
    "                    \"ADVAE\":\n",
    "                    tf.keras.Sequential(\n",
    "                        [\n",
    "                            tf.keras.layers.InputLayer(input_shape=(NUM_LATENT_DIM,)),\n",
    "                            tf.keras.layers.Dense(units=7*7*32, activation=tf.nn.relu),\n",
    "                            tf.keras.layers.Reshape(target_shape=(7, 7, 32)),\n",
    "                            tf.keras.layers.Conv2DTranspose(\n",
    "                                filters=64, kernel_size=3, strides=2, padding='same',\n",
    "                                activation='relu'),\n",
    "                            tf.keras.layers.Conv2DTranspose(\n",
    "                                filters=32, kernel_size=3, strides=2, padding='same',\n",
    "                                activation='relu'),\n",
    "                            # No activation\n",
    "                            tf.keras.layers.Conv2DTranspose(\n",
    "                                filters=1, kernel_size=3, strides=1, padding='same'),\n",
    "                        ]\n",
    "                        )\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ZTDRs7KORnM"
   },
   "outputs": [],
   "source": [
    "def setupCUDA(verbose, device_num):\n",
    "  \"\"\" Setup CUDA Environment to utalise specified GPUs and curtail memory growth\"\"\"\n",
    "\n",
    "  os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(device_num)\n",
    "\n",
    "  physical_devices = tf.config.list_physical_devices('GPU')\n",
    "  try:\n",
    "          tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "  except:\n",
    "\n",
    "       # Invalid device or cannot modify virtual devices once initialized.\n",
    "        pass\n",
    "  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "  if verbose:\n",
    "          tf.config.list_physical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BhGWVyZAOtJi"
   },
   "outputs": [],
   "source": [
    "def calculate_real_loss(data):\n",
    "  return tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.SUM, from_logits=True)(tf.ones_like(data), data)\n",
    "\n",
    "def calculate_fake_loss(data):\n",
    "  return tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.SUM, from_logits=True)(tf.zeros_like(data), data)\n",
    "\n",
    "def gan(real_data, fake_data):\n",
    "\n",
    "  real_loss = calculate_real_loss(real_data)\n",
    "  fake_losses = []\n",
    "\n",
    "  for data in fake_data:\n",
    "    fake_losses.append(calculate_fake_loss(data))\n",
    "\n",
    "  return [np.sum(fake_losses), real_loss]\n",
    "\n",
    "def compute_loss_gan(model, x):\n",
    "  \n",
    "  noise = tf.random.normal([BATCH_SIZE, NUM_LATENT_DIM])\n",
    "  generated_x = model.generate(noise)\n",
    "  \n",
    "  real_output = model.discriminate(x)\n",
    "  fake_output = model.discriminate(generated_x)\n",
    "  \n",
    "  return [model.generator_loss(fake_output), model.discriminator_loss(real_output, fake_output)]\n",
    "\n",
    "def vae(model, x):\n",
    "  \n",
    "  mean, logvar = model.encode(x)\n",
    "  z = model.reparameterize(mean, logvar)\n",
    "  x_logit = model.decode(z)\n",
    "  \n",
    "  logpz   = log_normal_pdf(z, 0., 0.)\n",
    "  logqz_x = log_normal_pdf(z, mean, logvar)\n",
    "    \n",
    "  return x_logit, x, logpz, logqz_x\n",
    "\n",
    "def compute_loss_vae(model, x):\n",
    "  \n",
    "  x_logit, x, logpz, logqz_x = vae(model, x)\n",
    "  \n",
    "  cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n",
    "  logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
    "  \n",
    "  return [-tf.reduce_mean(logpx_z + logpz - logqz_x)]\n",
    "\n",
    "def compute_loss_vaegan(model, x):\n",
    "\n",
    "  x_logit, x, logpz, logqz_x = vae(model, x)\n",
    "    \n",
    "  real_features = model.discrim_l(x)\n",
    "  fake_features = model.discrim_l(x_logit)\n",
    "  \n",
    "  logpx_z       = tf.reduce_sum((fake_features-real_features)**2)\n",
    "\n",
    "  noise         = tf.random.normal([x.shape[0], NUM_LATENT_DIM])\n",
    "\n",
    "  real_output   = model.discriminate(x)\n",
    "  fake_x_output = model.discriminate(x_logit)\n",
    "  fake_z_output = model.discriminate(model.decode(noise))\n",
    "\n",
    "  l_prior       = -tf.reduce_sum(logpz - logqz_x)\n",
    "  l_dis         = logpx_z\n",
    "  l_gan         = np.sum(gan(real_output, [fake_x_output, fake_z_output]))\n",
    "  \n",
    "  enc_loss      = l_prior + l_dis\n",
    "  dec_los       = l_dis   - l_gan\n",
    "  dis_loss      = l_gan\n",
    "  \n",
    "  return [enc_loss, dec_los, dis_loss]\n",
    "\n",
    "def compute_loss_advae(model, x):\n",
    "\n",
    "  z = model.encode_(x)\n",
    "  x_logit = model.decode(z)\n",
    "    \n",
    "  cross_ent = tf.reduce_sum((x-x_logit)**2)\n",
    "\n",
    "  fake_z = z\n",
    "  z      = np.random.normal(size = z.shape)\n",
    "\n",
    "  real_output = model.discriminate(z)\n",
    "  fake_output = model.discriminate(fake_z)\n",
    "  \n",
    "  l_reconstruct = cross_ent\n",
    "  l_regularise  = np.sum(gan(real_output, [fake_output]))\n",
    "\n",
    "  return [l_reconstruct, l_regularise]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zSmvOZzmOzi4"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step_vae(model, x, optimizers, input_size):\n",
    "  \"\"\"Executes one training step and returns the loss.\n",
    "\n",
    "  This function computes the loss and gradients, and uses the latter to\n",
    "  update the model's parameters.\n",
    "  \"\"\"\n",
    "  with tf.GradientTape() as tape:\n",
    "    losses = compute_loss_vae(model, x)\n",
    "    \n",
    "  gradients = []\n",
    "  gradients.append(tape.gradient(losses[0], model.trainable_variables))\n",
    "  \n",
    "  optimizers[0].apply_gradients(zip(gradients[0], model.trainable_variables))\n",
    "\n",
    "@tf.function\n",
    "def train_step_gan(model, x, optimizers, input_size):\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      losses = compute_loss_gan(model, x)\n",
    "    \n",
    "    gradients = []\n",
    "    gradients.append(gen_tape.gradient(losses[0], model.generator.trainable_variables))\n",
    "    gradients.append(disc_tape.gradient(losses[1], model.discriminator.trainable_variables))\n",
    "\n",
    "    optimizers[0].apply_gradients(zip(gradients[0], model.generator.trainable_variables))\n",
    "    optimizers[1].apply_gradients(zip(gradients[1], model.discriminator.trainable_variables))\n",
    "    \n",
    "@tf.function\n",
    "def train_step_vaegan(model, x, optimizers, input_size):\n",
    "  \n",
    "    with tf.GradientTape() as enc_tape, tf.GradientTape() as dec_tape, tf.GradientTape() as disc_tape:\n",
    "      losses = compute_loss_vaegan(model, x)\n",
    "      \n",
    "      #tf.clip_by_value(losses[0], 0, 100000);\n",
    "    \n",
    "    gradients = []\n",
    "    gradients.append(enc_tape.gradient(losses[0], model.encoder.trainable_variables))\n",
    "    gradients.append(dec_tape.gradient(losses[1], model.decoder.trainable_variables))\n",
    "    gradients.append(disc_tape.gradient(losses[2], model.discriminator.trainable_variables))\n",
    "\n",
    "    optimizers[0].apply_gradients(zip(gradients[0], model.encoder.trainable_variables))\n",
    "    optimizers[1].apply_gradients(zip(gradients[1], model.decoder.trainable_variables))\n",
    "    optimizers[2].apply_gradients(zip(gradients[2], model.discriminator.trainable_variables))\n",
    "    \n",
    "    return 0\n",
    "  \n",
    "@tf.function\n",
    "def train_step_advae(model, x, optimizers, input_size):\n",
    "  \n",
    "    with tf.GradientTape() as reconstruct_tape, tf.GradientTape() as regularise_tape:\n",
    "      losses = compute_loss_advae(model, x)\n",
    "    \n",
    "    gradients = []\n",
    "    gradients.append(reconstruct_tape.gradient(losses[0], model.encoder.trainable_variables + model.decoder.trainable_variables))\n",
    "    gradients.append(regularise_tape.gradient(losses[1], model.encoder.trainable_variables + model.discriminator.trainable_variables))\n",
    "\n",
    "    optimizers[0].apply_gradients(zip(gradients[0], model.encoder.trainable_variables + model.decoder.trainable_variables))\n",
    "    optimizers[1].apply_gradients(zip(gradients[1], model.encoder.trainable_variables + model.discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4c6v27PNOl2A"
   },
   "outputs": [],
   "source": [
    "def preprocess_images(images):\n",
    "  images = images.reshape((images.shape[0], 28, 28, 1)) / 255.\n",
    "  return np.where(images > .5, 1.0, 0.0).astype('float32')\n",
    "\n",
    "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
    "  log2pi = tf.math.log(2. * np.pi)\n",
    "  return tf.reduce_sum(\n",
    "      -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
    "      axis=raxis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NdCRnChEO8FY"
   },
   "outputs": [],
   "source": [
    "def generate_and_save_images_vae(model, epoch, test_sample, reconstruct = None):\n",
    "  mean, logvar = model.encode(test_sample)\n",
    "  z = model.reparameterize(mean, logvar)\n",
    "\n",
    "  if (reconstruct != None):\n",
    "    z = None;\n",
    "\n",
    "  predictions = model.sample(z)\n",
    "  fig = plt.figure(figsize=(4, 4))\n",
    "  \n",
    "  for i in range(predictions.shape[0]):\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    plt.imshow(predictions[i, :, :, 0], cmap='gray')\n",
    "    plt.axis('off')\n",
    "    \n",
    "  try:\n",
    "    os.mkdirs('./drive/MyDrive/tests/vae/')\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "  plt.savefig('./drive/MyDrive/tests/vae/image_at_epoch_{:04d}.png'.format(epoch))\n",
    "\n",
    "def generate_and_save_images_advae(model, epoch, test_sample):\n",
    "\n",
    "  z = model.encode_(test_sample)\n",
    "  predictions = model.sample(z)\n",
    "\n",
    "  fig = plt.figure(figsize=(4, 4))\n",
    "  \n",
    "  for i in range(predictions.shape[0]):\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    plt.imshow(predictions[i, :, :, 0], cmap='gray')\n",
    "    plt.axis('off')\n",
    "    \n",
    "  try:\n",
    "    os.mkdirs('./drive/MyDrive/tests/advae/')\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "  plt.savefig('./drive/MyDrive/tests/advae/image_at_epoch_{:04d}.png'.format(epoch))\n",
    "  \n",
    "def generate_and_save_images_vaegan(model, epoch, test_sample):\n",
    "  mean, logvar = model.encode(test_sample)\n",
    "  z = model.reparameterize(mean, logvar)\n",
    "  predictions = model.sample(z)\n",
    "  fig = plt.figure(figsize=(4, 4))\n",
    "  \n",
    "  for i in range(predictions.shape[0]):\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    plt.imshow(predictions[i, :, :, 0], cmap='gray')\n",
    "    plt.axis('off')\n",
    "    \n",
    "  try:\n",
    "    os.mkdirs('./drive/MyDrive/tests/vae/')\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "  plt.savefig('./drive/MyDrive/tests/vaegan/image_at_epoch_{:04d}.png'.format(epoch))\n",
    "  \n",
    "def generate_and_save_images_gan(model, epoch, test_sample):\n",
    "  \n",
    "  NUM_LATENT_DIM =  100\n",
    "  noise = tf.random.normal([len(test_sample), NUM_LATENT_DIM])\n",
    "  predictions = model.generate(noise)\n",
    "    \n",
    "  fig = plt.figure(figsize=(4, 4))\n",
    "  \n",
    "  try:\n",
    "    os.mkdirs('./drive/MyDrive/tests/gan/')\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "  for i in range(predictions.shape[0]): #predictions.shape[0]):\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    plt.imshow(predictions[i, :, :, 0], cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "  plt.savefig('./drive/MyDrive/tests/gan/image_at_epoch_{:04d}.png'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aCgFNiJ8QMwp",
    "outputId": "9a47dc83-efea-4f44-a4d3-89d90a27c949"
   },
   "outputs": [],
   "source": [
    "MODE = int(MODEL_TYPE)\n",
    "setupCUDA(1, DEVICE_NUM)\n",
    "\n",
    "tf.random.set_seed(123489)\n",
    "\n",
    "(train_images, _), (test_images, _) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_images     = preprocess_images(train_images)\n",
    "test_images      = preprocess_images(test_images)\n",
    "\n",
    "train_dataset    = (tf.data.Dataset.from_tensor_slices(train_images)\n",
    "         .shuffle(NUM_TRAINING_EXAMPLES).batch(BATCH_SIZE))\n",
    "test_dataset     = (tf.data.Dataset.from_tensor_slices(test_images)\n",
    "         .shuffle(NUM_TESTING_EXAMPLES).batch(BATCH_SIZE))\n",
    "  \n",
    "if (MODE == 0):\n",
    "  name           = \"VAE\"\n",
    "  loss_function  = compute_loss_vae\n",
    "  train_step     = train_step_vae\n",
    "  learning_rates = [1e-4]\n",
    "  plot_function  = generate_and_save_images_vae\n",
    "    \n",
    "elif (MODE == 1):\n",
    "  name           = \"GAN\"\n",
    "  loss_function  = compute_loss_gan\n",
    "  train_step     = train_step_gan\n",
    "  learning_rates = [1e-4, 1e-4]\n",
    "  plot_function  = generate_and_save_images_gan\n",
    "\n",
    "elif (MODE == 2):\n",
    "  name           = \"VAEGAN\"\n",
    "  loss_function  = compute_loss_vaegan\n",
    "  train_step     = train_step_vaegan\n",
    "  learning_rates = [1e-4, 1e-4, 1e-4]\n",
    "  plot_function  = generate_and_save_images_vaegan\n",
    "    \n",
    "elif (MODE == 3):\n",
    "  name           = \"ADVAE\"\n",
    "  loss_function  = compute_loss_advae\n",
    "  train_step     = train_step_advae\n",
    "  learning_rates = [1e-4, 1e-4]\n",
    "  plot_function  = generate_and_save_images_advae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pNmqNsdTP885"
   },
   "outputs": [],
   "source": [
    "class CVAE(tf.keras.Model):\n",
    "  \"\"\"Convolutional variational autoencoder.\"\"\"\n",
    "\n",
    "  def __init__(self, NUM_LATENT_DIM, VAEGAN_LAYER, NAME):\n",
    "    super(CVAE, self).__init__()\n",
    "    \n",
    "    self.NUM_LATENT_DIM = NUM_LATENT_DIM #Number of latent dimesions in vae and vaegan bottleneck, and number of input noise values in GAN\n",
    "    self.VAEGAN_LAYER   = VAEGAN_LAYER   #Only used in VAEGAN mode, layer of disc and enc on which to perform feature wise comparison\n",
    "    \n",
    "    self.encoder       = encoders      [NAME]\n",
    "    self.decoder       = decoders      [NAME]\n",
    "    self.discriminator = discriminators[NAME]\n",
    "    \n",
    "    self.generator = tf.keras.Sequential(\n",
    "      [ \n",
    "        tf.keras.layers.InputLayer(input_shape=(NUM_LATENT_DIM,)),\n",
    "        tf.keras.layers.Dense(units=7*7*256, use_bias = False),\n",
    "        #tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.LeakyReLU(),\n",
    "\n",
    "        tf.keras.layers.Reshape(target_shape=(7, 7, 256)),\n",
    "        tf.keras.layers.Conv2DTranspose(\n",
    "          filters=128, kernel_size=5, strides=1, padding='same', use_bias = False),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.LeakyReLU(),\n",
    "        tf.keras.layers.Conv2DTranspose(\n",
    "          filters=64, kernel_size=5, strides=2, padding='same', use_bias = False),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.LeakyReLU(),\n",
    "        tf.keras.layers.Conv2DTranspose(\n",
    "          filters=1, kernel_size=5, strides=2, padding='same', use_bias = False, activation = tf.keras.layers.LeakyReLU(alpha=0.01)),\n",
    "      ]\n",
    "    )\n",
    "    \n",
    "    self.dis_layer = tf.keras.models.Model(\n",
    "      inputs=self.discriminator.inputs,\n",
    "      outputs=self.discriminator.layers[VAEGAN_LAYER].output,\n",
    "    )\n",
    "\n",
    "    self.enc_layer = tf.keras.models.Model(\n",
    "      inputs=self.encoder.inputs,\n",
    "      outputs=self.encoder.layers[VAEGAN_LAYER].output,\n",
    "    )\n",
    "\n",
    "  @tf.function\n",
    "  def sample(self, eps=None):\n",
    "    if eps is None:\n",
    "      eps = tf.random.normal(shape=(NUM_PLOT, self.NUM_LATENT_DIM))\n",
    "    return self.decode(eps, apply_sigmoid=True)\n",
    "\n",
    "  def encode_l(self, x):\n",
    "\n",
    "    return self.enc_layer(x)\n",
    "\n",
    "  def discrim_l(self, x):\n",
    "    \n",
    "    return self.dis_layer(x)\n",
    "\n",
    "  def encode(self, x):\n",
    "    mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
    "    return mean, logvar\n",
    "\n",
    "  def encode_(self, x):\n",
    "    return self.encoder(x)\n",
    "\n",
    "  def reparameterize(self, mean, logvar):\n",
    "    eps = tf.random.normal(shape=mean.shape)\n",
    "    return eps * tf.exp(logvar * .5) + mean\n",
    "\n",
    "  def decode(self, z, apply_sigmoid=False):\n",
    "    logits = self.decoder(z)\n",
    "    if apply_sigmoid:\n",
    "      probs = tf.sigmoid(logits)\n",
    "      return probs\n",
    "    return logits\n",
    "  \n",
    "  def generate(self, z):\n",
    "    logits = self.generator(z)\n",
    "    return logits    \n",
    "\n",
    "  def discriminate(self, x):\n",
    "    logits = self.discriminator(x)\n",
    "    return logits\n",
    "\n",
    "  def discriminator_loss(self, real_output, fake_output):\n",
    "    real_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "  def generator_loss(self, fake_output):\n",
    "    return tf.keras.losses.BinaryCrossentropy(from_logits=True)(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9XFqYT6vQcIM"
   },
   "outputs": [],
   "source": [
    "model = CVAE(NUM_LATENT_DIM, VAEGAN_LAYER, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pAz1Te7r8qmq"
   },
   "outputs": [],
   "source": [
    "optimizers = []\n",
    "for rate in learning_rates:\n",
    "  optimizers.append(tf.keras.optimizers.Adam(rate))\n",
    "    \n",
    "# Pick a sample of the test set for generating output images\n",
    "assert BATCH_SIZE >= NUM_PLOT\n",
    "for test_batch in test_dataset.take(1):\n",
    "  test_sample = test_batch[0:NUM_PLOT, :, :, :]\n",
    "  \n",
    "plot_function(model, 0, test_sample)\n",
    "  \n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "  start_time = time.time()\n",
    "    \n",
    "  for train_x in train_dataset:\n",
    "      train_step(model, train_x, optimizers, input_size)\n",
    "      \n",
    "  end_time = time.time()\n",
    "  \n",
    "  loss_objects = []\n",
    "  for i in range(len(learning_rates)):\n",
    "    loss_objects.append(tf.keras.metrics.Mean())\n",
    "    \n",
    "  for test_x in test_dataset:\n",
    "    losses = loss_function(model, test_x)\n",
    "    for i in range(len(learning_rates)):\n",
    "      loss_objects[i](losses[i])\n",
    "                \n",
    "  print('Epoch: {} time elapse for current epoch: {}'.format(epoch, end_time - start_time))\n",
    "  print('Losses:')\n",
    "  for i in range(len(learning_rates)):\n",
    "    print(loss_objects[i].result())\n",
    "\n",
    "  plot_function(model, epoch, test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ev5XEtjPmmUk"
   },
   "outputs": [],
   "source": [
    "plot_function(model, 100, test_sample, reconstruct = True)\n",
    "plot_function(model, 100, test_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sZgEZUPMsr0e"
   },
   "outputs": [],
   "source": [
    "from sklearn import manifold\n",
    "import cv2\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "\n",
    "def imscatter(x, y, ax, imageData, zoom):\n",
    "    images = []\n",
    "    for i, img in enumerate(imageData):\n",
    "        x0, y0 = x[i], y[i]\n",
    "        # Convert to image\n",
    "        img = img.reshape([img.shape[0],img.shape[1]])\n",
    "        img = cv2.cvtColor(img,cv2.COLOR_GRAY2RGB)\n",
    "        # Note: OpenCV uses BGR and plt uses RGB\n",
    "        image = OffsetImage(img, zoom=zoom)\n",
    "        ab = AnnotationBbox(image, (x0, y0), xycoords='data', frameon=False)\n",
    "        images.append(ax.add_artist(ab))\n",
    "    \n",
    "    ax.update_datalim(np.column_stack([x, y]))\n",
    "    ax.autoscale()\n",
    "\n",
    "\n",
    "def computeTSNEProjectionOfLatentSpace(X, encoder, display=True):\n",
    "    # Compute latent space representation\n",
    "    print(\"Computing latent space projection...\")\n",
    "    X_encoded = encoder.predict(X)\n",
    "\n",
    "    # Compute t-SNE embedding of latent space\n",
    "    print(\"Computing t-SNE embedding...\")\n",
    "    tsne = manifold.TSNE(n_components=2, init='pca', random_state=0)\n",
    "    X_tsne = tsne.fit_transform(X_encoded)\n",
    "\n",
    "    # Plot images according to t-sne embedding\n",
    "    if display:\n",
    "        print(\"Plotting t-SNE visualization...\")\n",
    "        fig, ax = plt.subplots()\n",
    "        imscatter(X_tsne[:, 0], X_tsne[:, 1], imageData=X, ax=ax, zoom=0.6)\n",
    "        plt.show()\n",
    "    else:\n",
    "        return X_tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LDJapDPhzHDm"
   },
   "outputs": [],
   "source": [
    " X_tsne = computeTSNEProjectionOfLatentSpace(test_images, model.encoder, display = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ix745Lys9rpN"
   },
   "outputs": [],
   "source": [
    "\n",
    "r_test_images = test_images[1:200]\n",
    "r_X_tsne = X_tsne[1:200]\n",
    "\n",
    "print(\"Plotting t-SNE visualization...\")\n",
    "fig, ax = plt.subplots()\n",
    "imscatter(r_X_tsne[:, 0], r_X_tsne[:, 1], imageData=r_test_images, ax=ax, zoom=0.6)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LdoVZhNiKcv0"
   },
   "outputs": [],
   "source": [
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "def visualizeInterpolation(start, end, model, encode, decode, save=False, nbSteps=5):\n",
    "    print(\"Generating interpolations...\")\n",
    "\n",
    "    # Create micro batch\n",
    "    X = np.array([start,end])\n",
    "\n",
    "    # Compute latent space projection\n",
    "    mean, logvar = encode(np.array([start]))\n",
    "    latentStart = model.reparameterize(mean, logvar)\n",
    "\n",
    "    mean, logvar = encode(np.array([end]))\n",
    "    latentEnd = model.reparameterize(mean, logvar)\n",
    "\n",
    "    # Get original image for comparison\n",
    "    startImage, endImage = X\n",
    "\n",
    "    vectors = []\n",
    "    normalImages = []\n",
    "    #Linear interpolation\n",
    "    alphaValues = np.linspace(0, 1, nbSteps)\n",
    "\n",
    "    reconstructions = []\n",
    "    for alpha in alphaValues:\n",
    "        # Latent space interpolation\n",
    "        vector = latentStart*(1-alpha) + latentEnd*alpha\n",
    "        vectors.append(vector)\n",
    "        # Image space interpolation\n",
    "        blendImage = cv2.addWeighted(startImage,1-alpha,endImage,alpha,0)\n",
    "        normalImages.append(blendImage)\n",
    "        reconstructions.append(model.decode(vector, apply_sigmoid=True).numpy())\n",
    "\n",
    "    np.array(reconstructions)\n",
    "    # Put final image together\n",
    "    resultLatent = None\n",
    "    resultImage = None\n",
    "\n",
    "    if save:\n",
    "        hashName = ''.join(random.choice(string.lowercase) for i in range(3))\n",
    "\n",
    "    for i in range(len(reconstructions)):\n",
    "        interpolatedImage = normalImages[i]*255\n",
    "        interpolatedImage = cv2.resize(interpolatedImage,(50,50))\n",
    "        interpolatedImage = interpolatedImage.astype(np.uint8)\n",
    "        resultImage = interpolatedImage if resultImage is None else np.hstack([resultImage,interpolatedImage])\n",
    "\n",
    "        reconstructedImage = reconstructions[i]*255.\n",
    "        reconstructedImage = reconstructedImage.reshape([28,28])\n",
    "        reconstructedImage = cv2.resize(reconstructedImage,(50,50))\n",
    "        reconstructedImage = reconstructedImage.astype(np.uint8)\n",
    "        resultLatent = reconstructedImage if resultLatent is None else np.hstack([resultLatent,reconstructedImage])\n",
    "    \n",
    "        if save:\n",
    "            cv2.imwrite(visuals_path+\"{}_{}.png\".format(hashName,i),np.hstack([interpolatedImage,reconstructedImage]))\n",
    "\n",
    "        result = np.vstack([resultImage,resultLatent])\n",
    "\n",
    "    if not save:\n",
    "        cv2_imshow(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GhT80_gGK0x9"
   },
   "outputs": [],
   "source": [
    "print(np.argmax(test_images[0]), np.argmax(test_images[3]))\n",
    "\n",
    "visualizeInterpolation((test_images[56]/np.max(test_images[56])), (test_images[87]/np.max(test_images[87])), model, model.encode, model.decoder, save=False, nbSteps=5)\n",
    "visualizeInterpolation((test_images[1]/np.max(test_images[1])), (test_images[9]/np.max(test_images[9])), model, model.encode, model.decoder, save=False, nbSteps=5)\n",
    "visualizeInterpolation((test_images[0]/np.max(test_images[0])), (test_images[7]/np.max(test_images[7])), model, model.encode, model.decoder, save=False, nbSteps=5)\n",
    "visualizeInterpolation((test_images[4]/np.max(test_images[4])), (test_images[21]/np.max(test_images[21])), model, model.encode, model.decoder, save=False, nbSteps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Vaegan.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
